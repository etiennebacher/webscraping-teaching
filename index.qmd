---
title: "Webscraping with RSelenium"
subtitle: "Automate your browser actions"
author: "Etienne Bacher"
date: "`r Sys.Date()`"
date-format: long
institute: "LISER"

format: 
 revealjs:
   incremental: false  
   theme: [moon, custom.scss]
   pdf-separate-fragments: true
   preview-links: true
   strip-comments: true
   highlight-style: atom-one
   
execute:
  eval: false
  echo: true
---

<!-- Print to PDF -->
<!-- Follow this: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf -->
<!-- Use chrome and not firefox -->


## Introduction

<br>

::: {.callout-important icon="false" appearance="simple"}
Do you really need scraping?
:::

Before scraping: is there an API?

::: increment
-   if yes, is there a package?

    -   if yes, use the package (e.g `guardian`, `WDI`)

    -   if no, build the API queries yourself with `httr`

-   if no, scrape (politely)
:::

<!-- https://talks.andrewheiss.com/2022-seacen/presentation/#/general-principles -->

## Introduction

<br>

Scraping can be divided in two steps:

::: incremental
1.  getting the HTML that contains the information
2.  cleaning the HTML to extract the information we want
:::

. . .

<br>

These 2 steps don't necessarily require the same tools, and *shouldn't
be made at the same time*.

## Introduction

<br>

Why?

<br>

Webscraping takes time, and a lot of things can happen:

* your Internet connection goes down;
* the website goes down;
* any other random reason

<br>

If this happen and you didn't save your progress, you lose everything.

We don't have plenty of time, but we have plenty of disk storage. Use it!


## Introduction

<br> <br> <br>

Here, we will focus mostly on **how to obtain the HTML code you
need on dynamic pages?**

<br>

(And a bit on how to clean this HTML)



# Static and dynamic pages 

## Static and dynamic pages

<br><br>

The web works with 3 languages:

::: incremental
-   HTML: content and structure of the page
-   CSS: style of the page
-   JavaScript: interactions with the page
:::


## Static and dynamic pages

<br><br>

The web works with 3 languages:

-   HTML: content and structure of the page

-   CSS: style of the page

-   [**JavaScript**: interactions with the page]{style="color: beige;"}


## Static vs dynamic

<br>

**Static webpage**:

-   all the information is loaded with the page;
-   changing a parameter modifies the URL

Examples: [Wikipedia](https://en.wikipedia.org/wiki/Web_scraping), [IMDB](https://www.imdb.com/name/nm0001392/?ref_=nv_sr_srsg_0).

<br>

. . .

**Dynamic webpage**: the website uses JavaScript to fetch data from
their server and *dynamically* update the page.

Example: [Premier League stats](https://www.premierleague.com/stats/top/players/goal_assist).

# Why is it harder to do webscraping with dynamic pages?

------------

<br>

Webscraping a static website can be quite simple:

-   you get a list of URLs;
-   download the HTML for each of them;
-   read and clean the HTML

and that's it.

<br>

. . .

This is "easy" because you can differentiate two pages with different
content just by looking at their URL.

------------

Example: elections results in Spain from the website of [El Pais](https://resultados.elpais.com/elecciones/2019/municipales/)

![](img/elpais1.png){width="60%"}

![](img/elpais2.png){width="60%"}

------------

<br> <br> 

Of course, static webscraping can be challenging because we have to write good 
loops, good error handling, the HTML itself can be hard to clean, etc.

. . .

<br> 

But in dynamic pages, there's no obvious way to see that the inputs are
different (see [Premier League stats](https://www.premierleague.com/stats/top/players/goal_assist)).

------------

<br> <br>

So it seems that the only way to get the data is to go manually through
all pages to get the HTML.

<br> <br>

. . .

*350h later...*

# (R)Selenium

## Idea

Idea: control the browser from the command line.

<br>

. . .

*"I wish I could click on this button to open a modal"*

```{r}
remote_driver$
  findElement(using = "css", value = ".my-button")$
  clickElement()
```

<br>

. . .

*"I wish I could fill these inputs to automatically connect"*

```{r}
remote_driver$
  findElement(using = "id", value = "password")$
  sendKeysToElement(list("my_super_secret_password"))
```

------------

<br> 

Almost everything you can do "by hand" in a browser, you can reproduce
with Selenium:

. . .

::: columns
::: {.column width="45%"}
-   open a browser

-   click on something

-   enter values

-   go to previous/next page

-   refresh the page

-   get all the HTML that is currently displayed
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
-   `open()` / `navigate()`

-   `clickElement()`

-   `sendKeysToElement()`

-   `goBack()` / `goForward()`

-   `refresh()`

-   `getPageSource()`
:::
:::

# Get started

## Get started

<br> 

In the beginning there was ~~light~~ `rsDriver()`:

```{r}
# if not already installed
# install.packages("RSelenium")
library(RSelenium)

driver <- rsDriver(browser = "firefox") # can also be chrome
remote_driver <- driver[["client"]]
```

. . .

<br> 

This will print a bunch of messages and open a "marionette browser".

![](img/marionette2.png)


## Get started

From now on, the main thing is to call `<function>()` starting with
`remote_driver$`[^1].

[^1]: Or whatever you called it in the previous step.

<img src="img/rsdriver_funcs.png" alt="drawing" style="width:75%; text-align: center"/>

# Exercise 1


## Exercise 1

<br> 

**Objective:** get the list of core contributors to R located
[here](https://www.r-project.org/contributors.html).

. . .

<br> 

How would you do it by hand?

-   open the browser;
-   go to <https://r-project.org>;
-   in the left sidebar, click on the link "Contributors";

and voilà!

. . .

<br>

How can we do these steps programmatically?


## Open the browser and navigate

```{r}
remote_driver$navigate("https://r-project.org")
```

<img src="img/rproject.png" alt="drawing" style="width:75% !important; text-align: center !important"/>



## Click on "Contributors"

<br>

This requires two things:

1.  find the element
2.  click on it

<br>

**How to find an element?**

-   Humans -> eyes

-   Computers -> HTML/CSS

---

To find the element, we need to open the console to see the structure of
the page:

-   right-click -> "Inspect"
-   `Ctrl` + `Shift` + `C`

. . .

![](img/console.png){width="75%"}

------------

Then, hover the element we're interested in: the link "Contributors".

![](img/console_2.png)

------------

How can we find this link with `RSelenium`?

```{r}
?RSelenium::remoteDriver()
```


<br> 

-> `findElement`

::: columns
::: {.column width="45%"}
-   class name ❌
-   id ❌
-   name ❌
-   tag name ❌
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
-   css selector ✔️
-   link text ✔️
-   partial link text ✔️
-   xpath ✔️
:::
:::

------------

All of these work:

```{r}
remote_driver$
  findElement("link text", "Contributors")$
  clickElement()

remote_driver$
  findElement("partial link text", "Contributors")$
  clickElement()

remote_driver$
  findElement("xpath", "/html/body/div/div[1]/div[1]/div/div[1]/ul/li[3]/a")$
  clickElement()

remote_driver$
  findElement("css selector", "div.col-xs-6:nth-child(1) > ul:nth-child(6) > li:nth-child(3) > a:nth-child(1)")$
  clickElement()
```

. . .

::: callout-tip
You can check that you found the right element by highlighting it with
`highlightElement()`.
:::

------------

We are now on the right page!

![](img/contributors.png)

------------

<br><br>

Last step: obtain the HTML of the page.

```{r}
remote_driver$getPageSource()
```

<br>

. . .

To read it with the package `rvest`:

```{r}
x <- remote_driver$getPageSource()[[1]]
rvest::read_html(x)
```


##  {#contributors-last-step}

<br>

Do we read the HTML and extract the information in the same script?

. . .

<br>


**No!**

Instead, we save the HTML in an external file, and we will be able to
access it in another script (and offline) to manipulate it as we want.

<!-- ^[Although, in this case, it wouldn't cost too much to treat it directly in the same script.] -->

```{r}
write(x, file = "contributors.html")
# Later and in another script
rvest::read_html("contributors.html")
```

<br>

Click [here](#contributors-results) to see the results.

# Exercise 2: a harder & real-life example

------------

<br>


The previous example was not a *dynamic* page: we could have used the
link to the page and apply webscraping methods for static webpages.

<br>

```{r}
rvest::read_html("https://www.r-project.org/contributors.html")
```

. . .

<br>

Let's now dive into a more complex example, where RSelenium is the only
way to obtain the data.


## Before using RSelenium

<br>

*Using RSelenium is slower than using "classic" scraping methods*, so
it's important to check all possibilities before using it.

<br>

. . .

Use Selenium if:

-   the HTML you want is not directly accessible, i.e needs some
    interactions (clicking on a button, connect to a website...),

-   the URL doesn't change with the inputs,

-   you can't access the data directly in the "network" tab of the
    console and you can't reproduce the `POST` request.


## Example: Sao Paulo immigration museum

<br>

[Open website](http://www.inci.org.br/acervodigital/livros.php)

<br>

Steps:

1. list all interactions we need to do
1. check that we need Selenium 
1. make an example
1. generalize and polish the code



## List all interactions

<br><br>

1. Open the website
1. Enter "PORTUGUESA" in the input box
1. Wait a bit for the page to load
1. Open every modal "Ver Mais"



## Check that we need Selenium

1. Is there an API?

::: {.fragment .fade-in fragment-index=1}
::: {.fragment .highlight-current-blue fragment-index=1}
Not that I know of (and let's assume that there isn't one).
:::
:::

2. Does the URL change when we enter inputs or click somewhere?

::: {.fragment .fade-in fragment-index=2}
::: {.fragment .highlight-current-blue fragment-index=2}
No.
:::
:::

3. Can we get the data through the "Network" tab?

::: {.fragment .fade-in fragment-index=3}
::: {.fragment .highlight-current-blue fragment-index=3}
Yes but we still need RSelenium to change pages (and this is just training
anyway).
:::
:::



## Make an example {auto-animate="true"}

Initiate the remote driver and go to the website:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)
```


## Make an example {auto-animate=true}

Fill the field "NACIONALIDADE":

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))
```

## Make an example {auto-animate=true}

Find the button "Pesquisar" and click it:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()
```


## Make an example {auto-animate=true}

Find the button "Ver Mais" and click it:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()

# Find the button "Ver Mais" and click it
remote_driver$
  findElement(using = 'id', value = "link_ver_detalhe")$
  clickElement()
```


## Make an example {auto-animate=true}

Get the HTML that is displayed:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()

# Find the button "Ver Mais" and click it
remote_driver$
  findElement(using = 'id', value = "link_ver_detalhe")$
  clickElement()

# Get the HTML that is displayed in the modal
x <- remote_driver$getPageSource()
```


## Problem

<br><br>

We got the content of the first modal, that's great!

<br>

Now we just need to replicate this for the other modals of the page.

<br>

. . .

**How can we distinguish one button "Ver Mais" from another?**


## Problem

<br>

To find the button "Ver Mais", we used the following code:

```{r}
remote_driver$
  findElement(using = 'id', value = "link_ver_detalhe")$
  clickElement()
```

<br>

But all buttons share the same `id`, so this code only selects the first button,
not the others.


## Solution

<br>

Use `findElements()` (and not `findElement()`).

<br>

This returns a list of elements, and we can then apply some function on each of
them in a loop:

```{r}
# Make the list of elements
buttons <- remote_driver$
  findElements(using = 'id', value = "link_ver_detalhe")

# Highlight each button one by one
for (i in seq_along(buttons)) {
  buttons[[i]]$highlightElement()
  Sys.sleep(1)
}
```


## Loop through modals

<br>

Now that we have a way to open each modal, we can make a loop to get the HTML
for each one:

```{r}
for (i in seq_along(buttons)) {
  
  # open the modal
  buttons[[i]]$clickElement()
  Sys.sleep(0.5)
  
  # get the HTML and save it
  tmp <- remote_driver$getPageSource()[[1]]
  write(tmp, file = paste0("data/modals/modal-", i, ".html"))
  
  # quit the modal (by pressing "Escape")
  body$sendKeysToElement(list(key = "escape"))
  
}

```

## Generalize for each page

<br>

Find the button to go to the next page:

<br>

```{r}
remote_driver$
  findElement("css", "#paginacao > div.btn:nth-child(4)")$
  highlightElement()
```


## Nested loops

<br>

We know how to:

* open the website
* search for the right inputs
* open each modal and get its content
* go to the next page

. . .

<br>

Next step: compile all of this and make nested loops!



## {auto-animate=true}

How many pages? **2348**

<br>

Pseudo-code:

```{r}
for (page_index in 1:2348) {
  
  # Find all buttons "Ver Mais" on the page 
  
  for (modal_index in buttons) {
    # open modal
    # get HTML and save it in an external file
    # leave modal
  }
  
  # Once all modals of a page have been scraped, go to the next page (except
  # if we're on the last page)
  
}
```


## {auto-animate=true}

How many pages? **2348**

<br>

Pseudo-code:

```{.r code-line-numbers="3-5"}
for (page_index in 1:2348) {
  
  # Find all buttons "Ver Mais" on the page 
  buttons <- remote_driver$
    findElements(using = 'id', value = "link_ver_detalhe")
  
  for (modal_index in buttons) {
    # open modal
    # get HTML and save it in an external file
    # leave modal
  }
  
  # Once all modals of a page have been scraped, go to the next page (except
  # if we're on the last page)
  
}
```


## {auto-animate=true}

How many pages? **2348**

<br>

Make the "modal loop":

```{.r code-line-numbers="7-22"}
for (page_index in 1:2348) {

  # Find all buttons "Ver Mais" on the page 
  buttons <- remote_driver$
    findElements(using = 'id', value = "link_ver_detalhe")
  
  for (modal_index in buttons) {
    # open modal
    buttons[[modal_index]]$clickElement()

    Sys.sleep(0.5)

    # Get the HTML and save it
    tmp <- remote_driver$getPageSource()[[1]]
    write(tmp, file = paste0("data/modals/modal-", modal_index, ".html"))

    # Leave the modal
    body <- remote_driver$findElement(using = "xpath", value = "/html/body")
    body$sendKeysToElement(list(key = "escape"))

    Sys.sleep(0.5)
  }
  
  # Once all modals of a page have been scraped, go to the next page (except
  # if we're on the last page)
  
}
```


## {auto-animate=true}

How many pages? **2348**

<br>

Make the "page loop":

```{.r code-line-numbers="25-33"}
for (page_index in 1:2348) {
  
  # Find all buttons "Ver Mais" on the page 
  buttons <- remote_driver$
    findElements(using = 'id', value = "link_ver_detalhe")
    
  for (modal_index in buttons) {
    # open modal
    buttons[[modal_index]]$clickElement()

    Sys.sleep(0.5)

    # Get the HTML and save it
    tmp <- remote_driver$getPageSource()[[1]]
    write(tmp, file = paste0("data/modals/page-", page_index, 
                             "-modal-", modal_index, ".html"))

    # Leave the modal
    body <- remote_driver$findElement(using = "xpath", value = "/html/body")
    body$sendKeysToElement(list(key = "escape"))

    Sys.sleep(0.5)
  }
  
  # When we got all modals of one page, go to the next page (except if we're on
  # the last one)
  if (page_index != 2348) {
    remote_driver$
      findElement("css", "#paginacao > div.btn:nth-child(4)")$
      clickElement()

    Sys.sleep(3)
  }
  
}
```



# Error handling

## Loading times

<br>

There are a few places where we need to wait a bit:

* after clicking on "Pesquisar" 
* after clicking on "Ver Mais"
* when we go to the next page

. . .

<br>


We must put some pauses between `RSelenium` actions. Otherwise it will error,
e.g if we try to click on a button that was loaded on the page yet.

. . .

<br>

{{< fa arrow-right >}} &nbsp; `Sys.sleep()`



## Catching errors


## Display and save information

<br>

**Webscraping takes time.** 

<br>

It is important to show and save information on how the webscraping is going so
that we know where it went wrong for debugging.

<br>

. . .

In our case:

* show which page is being scraped;
* show which modal of this page is being scraped;
* show the status of this scraping (success/failure).


## Display information

Use `message()` at several places in the loop to display information:

```{.r code-line-numbers="3,10,16"}
for (page_index in 1:2348) {
  
  message(paste("Start scraping of page", page_index))
   
  for (modal_index in buttons) {
    # open modal
    # get HTML and save it in an external file
    # leave modal
    
    message(paste("  Scraped modal", modal_index))
  }
  
  # Once all modals of a page have been scraped, go to the next page (except
  # if we're on the last page)
  
  message(paste("Finished scraping of page", page_index))
}
```




## Save information

<br><br>

Problem: what if the R session crashes?

<br>

We lose all messages!

<br>

. . .

Solution: show these messages **and** save them in an external file at the same
time.


## Save information

Example using the package `logger` (there are also `logging`, `futile.logger`,
etc.):

```{.r code-line-numbers="1,2,3,7,14,20"}
# save calls to message() in an external file
log_appender(appender_file("data/modals/00_logfile"))
log_messages()

for (page_index in 1:2348) {
  
  message(paste("Start scraping of page", page_index))
   
  for (modal_index in buttons) {
    # open modal
    # get HTML and save it in an external file
    # leave modal
    
    message(paste("  Scraped modal", modal_index))
  }
  
  # Once all modals of a page have been scraped, go to the next page (except
  # if we're on the last page)
  
  message(paste("Finished scraping of page", page_index))
}
```

## Save information

What does the output look like?

![](img/logfile_demo.png){width="600" height="600"}


# Now what?

---

<br> <br>

If everything went well, we now have a bunch of `.html` files in `data/modals`.

<br>

To clean them, we don't need `RSelenium` or an internet connection. These are just
text files, they are not "tied" to the website anymore.

<br>

It is also useful to keep them for reproducibility (same as when you keep the 
raw datasets in your project).


---

Make a function to clean the HTML. It returns a list containing a dataframe with
the personal info, and a dataframe with the "network" of the individual.

```{r}
extract_information <- function(raw_html) {
  
  # Extract the table "Registros relacionados"
  
  content <- raw_html %>%
    html_nodes("#detalhe_conteudo") %>%
    html_table() %>%
    purrr::pluck(1)
  
  relacionados <- content[16:nrow(content),] %>%
    mutate(
      across(
        .cols = everything(),
        .fns = ~ {ifelse(.x == "", NA, .x)}
      )
    )
  
  colnames(relacionados) <- c("Livro", "Pagina", "Familia", "Chegada",
                              "Sobrenome", "Nome", "Idade", "Sexo",
                              "Parentesco", "Nacionalidade",
                              "Vapor", "Est.Civil", "Religiao")
  
  
  # Extract text information from "registro de matricula" and create a
  # dataframe from it
  name_items <- raw_html %>%
    html_elements(xpath = '//*[@id="detalhe_conteudo"]/table[1]/tbody/tr/td/strong') %>%
    html_text2() %>%
    gsub("\\n", "", .) %>%
    strsplit(split = "\\t") %>%
    unlist()
  
  value_items <- raw_html %>%
    html_elements(xpath = '//*[@id="detalhe_conteudo"]/table[1]/tbody/tr/td/div') %>%
    html_text2()
  
  registro <- data.frame() %>%
    rbind(value_items) %>%
    as_tibble()
  
  colnames(registro) <- name_items
  
  return(
    list(
      main = registro,
      related = relacionados
    )
  )
  
}
```

---

Apply this function to all files:

<br>

```{r}
# Get all paths to the html files
list_html_files <- list.files("data/modals", pattern = "page",
                              full.names = TRUE)

# Apply the previous function to each of those file
list_out <- lapply(list_html_files, function(x) {
  read_html(x) |> 
    extract_information() 
})

# Aggregate the results in two (single) datasets
main <- data.table::rbindlist(purrr::map(list_out, 1)) |> 
  as_tibble()

relations <- data.table::rbindlist(purrr::map(list_out, 2)) |> 
  as_tibble()

```



# Summary

---

<br>

1. `Selenium` in general is a very useful tool but should be used in last resort:

  * APIs, packages
  * static webscraping
  * custom `POST` requests

. . .

2. In my (limited) experience:

  * 1/4 of the time is spent on making a small example work;
  * 1/4 of the time is spent on generalising this example (loops, etc.)
  * **1/2** of the time is spent on debugging.
  
<br>

. . .

***Catching errors and recording the scraping process IS important.***

---

## Ethics

<br>

Pay attention to a website's Terms of Use/Service.

<br>

Some websites explicitely say that you are not allowed to programmatically access
their resources.

![](img/terms-of-use.png)


## Final script

Also available on [Github]():

```{r}

```



# Thanks!

Source code for slides and exercises: 

[https://github.com/etiennebacher/webscraping-teaching](https://github.com/etiennebacher/webscraping-teaching)



# Appendix

## Appendix {#contributors-results}

For reference, here's the code to extract the list of contributors:

```{r appendix, echo=TRUE}
library(rvest)

html <- read_html("contributors.html") 

bullet_points <- html %>% 
  html_elements(css = "div.col-xs-12 > ul > li") %>% 
  html_text()

blockquote <- html %>% 
  html_elements(css = "div.col-xs-12.col-sm-7 > blockquote") %>% 
  html_text() %>% 
  strsplit(., split = ", ")

blockquote <- blockquote[[1]] %>% 
  gsub("\\r|\\n|\\.|and", "", .)

others <- html %>% 
  html_elements(xpath = "/html/body/div/div[1]/div[2]/p[5]") %>% 
  html_text() %>% 
  strsplit(., split = ", ")

others <- others[[1]] %>% 
  gsub("\\r|\\n|\\.|and", "", .)

all_contributors <- c(bullet_points, blockquote, others)
```


## Appendix

```{r appendix, eval=TRUE, echo = FALSE}
```

```{r eval=TRUE, echo = FALSE}
all_contributors[1:136] 
```

[Back](#contributors-last-step)



## Appendix

<br>

Bonus: get the data from each modal on the museum website by performing the `POST`
request yourself.

<br>

Go to the tab "Network" in the developer console, and then click on one of the
"Ver Mais" button to display the modal.


## Appendix

Clicking on this button triggers two `POST` requests: 

* one for the individual information;
* one for the "network" of the individual.

![](img/console_network.png)


## Appendix

<br>

Clicking on one of the `POST` requests displays important information: 

* the **request**, what we send to the server;
* the **response**, what the server sends back to us.

. . .

The request contains specific parameters needed to tell the server which data
we need.


## Appendix {auto-animate=true}

Let's rebuild this `POST` request from R using the package `httr`.

```{r}
library(httr)

x <- POST(
  "http://www.arquivoestado.sp.gov.br/site/acervo/memoria_do_imigrante/getHospedariaDetalhe",
  body = list(
    id = "92276"
  ),
  encode = "multipart"
)
```


## Appendix {auto-animate=true}

Extract the data from the server response:

```{r}
library(httr)
library(xml2)
library(jsonlite)

# make the POST request with the parameters needed
x <- POST(
  "http://www.arquivoestado.sp.gov.br/site/acervo/memoria_do_imigrante/getHospedariaDetalhe",
  body = list(
    id = "92276"
  ),
  encode = "multipart"
)

# convert output to a list
out <- as_list(content(x))

# convert output to a dataframe
fromJSON(unlist(out))$dados
```


## Your turn

<br>

Do the same thing with the second `POST` request, which has 3 parameters 
instead of one.
