---
title: "Webscraping with RSelenium"
subtitle: "Automate your browser actions"
author: "Etienne Bacher"
date: "`r Sys.Date()`"
institute: "LISER"

format: 
 revealjs:
   incremental: false  
   theme: [moon, custom.scss]
   pdf-separate-fragments: true
   preview-links: true
   strip-comments: true
   
execute:
  eval: false
  echo: true
---

<!-- Print to PDF -->
<!-- Follow this: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf -->
<!-- Use chrome and not firefox -->


## Introduction

::: {.callout-important icon="false" appearance="simple"}
Do you really need scraping?
:::

Before scraping: is there an API?

::: increment
-   if yes, is there a package?

    -   if yes, use the package (e.g `guardian`, `WDI`)

    -   if no, build the API queries yourself with `{httr2}`

-   if no, scrape (politely)
:::

<!-- https://talks.andrewheiss.com/2022-seacen/presentation/#/general-principles -->

------------

## Introduction

<br>

Scraping can be divided in two steps:

::: incremental
1.  getting the HTML that contains the information
2.  cleaning the HTML to extract the information we want
:::

. . .

<br>

These 2 steps don't necessarily require the same tools, and *shouldn't
be made at the same time*.

------------

## Introduction

<br> <br>

Here, we will focus on the first step: **how to obtain the HTML code you
need on dynamic pages?**

# Static and dynamic pages {.center}

------------

## Static and dynamic pages

<br>

The web works with 3 languages:

::: incremental
-   HTML: content and structure of the page
-   CSS: style of the page
-   JavaScript: interactions with the page
:::

------------

## Static and dynamic pages

<br>

The web works with 3 languages:

-   HTML: content and structure of the page

-   CSS: style of the page

-   [**JavaScript**: interactions with the page]{style="color: beige;"}

------------

## Static vs dynamic

**Static webpage**:

-   all the information is loaded with the page;
-   changing a parameter modifies the URL

Examples: Wikipedia, IMDB.

<br>

. . .

**Dynamic webpage**: the website uses JavaScript to fetch data from
their server and *dynamically* update the page.

Example: [Premier League stats](https://www.premierleague.com/stats/top/players/goal_assist).

# Why is it harder to do webscraping with dynamic pages?

------------

Webscraping a static website can be quite simple:

-   you get a list of URLs;
-   download the HTML for each of them;
-   read and clean the HTML

and that's it.

<br>

. . .

This is easy because you can differentiate two pages with different
content just by looking at their URL.

------------

Example:

------------

<br> 

Of course, static webscraping can be challenging because we have to write good 
loops, good error handling, the HTML itself can be hard to clean, etc.

. . .

<br> 

But in dynamic pages, there's no obvious way to see that the inputs are
different (see [Premier League stats](https://www.premierleague.com/stats/top/players/goal_assist)).

------------

<br> <br>

So it seems that the only way to get the data is to go manually through
all pages to get the HTML.

<br> <br>

. . .

*350h and 3 RAs later...*

# (R)Selenium

------------

## Idea

Idea: control the browser from the command line.

<br>

. . .

*"I wish I could click on this button to open a modal"*

```{r}
remote_driver$
  findElement(using = "css", value = ".my-button")$
  clickElement()
```

<br>

. . .

*"I wish I could fill these inputs to automatically connect"*

```{r}
remote_driver$
  findElement(using = "id", value = "password")$
  sendKeysToElement(list("my_super_secret_password"))
```

------------

Almost everything you can do "by hand" in a browser, you can reproduce
with Selenium:

. . .

::: columns
::: {.column width="45%"}
-   open a browser

-   click on something

-   enter values

-   go to previous/next page

<p style="margin-bottom: -1em"></p>
<br>

-   refresh the page

-   get all the HTML that is currently displayed
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
-   `open()` / `navigate()`

-   `clickElement()`

-   `sendKeysToElement()`

-   `goBack()` / `goForward()`

-   `refresh()`

-   `getPageSource()`
:::
:::

# Get started

------------

## Get started

In the beginning there was ~~light~~ `rsDriver()`:

```{r}
# if not already installed
# install.packages("RSelenium")
library(RSelenium)

driver <- rsDriver(browser = "firefox") # can also be chrome
remote_driver <- driver[["client"]]
```

. . .

This will print a bunch of messages and open a "marionette browser".

![](img/marionette2.png)

------------

## Get started

From now on, everything we do is calling `<function>()` starting with
`remote_driver$`[^1].

[^1]: Or whatever you called it in the previous step.

<img src="img/rsdriver_funcs.png" alt="drawing" style="width:75%; text-align: center"/>

# Exercise 1

------------

## Exercise 1

**Objective:** get the list of core contributors to R located
[here](https://www.r-project.org/contributors.html).

. . .

How would you do it by hand?

-   open the browser;
-   go to <https://r-project.org>;
-   in the left sidebar, click on the link "Contributors";

and voilà!

. . .

How can we do these steps programmatically?

------------

## Open the browser and navigate

```{r}
remote_driver$navigate("https://r-project.org")
```

<img src="img/rproject.png" alt="drawing" style="width:75% !important; text-align: center !important"/>


------------

## Click on "Contributors"

This requires two things:

1.  find the element
2.  click on it

**How to find an element?**

-   Humans -\> eyes

-   Computers -\> HTML/CSS

------------

To find the element, we need to open the console to see the structure of
the page:

-   right-click -\> "Inspect"
-   `Ctrl` + `Shift` + `C`

. . .

![](img/console.png){width="75%"}

------------

Then, hover the element we're interested in: the link "Contributors".

![](img/console_2.png)

------------

How can we find this with `RSelenium`?

```{r}
?RSelenium::remoteDriver
```

-\> `findElement`

::: columns
::: {.column width="45%"}
-   class name ❌
-   id ❌
-   name ❌
-   tag name ❌
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
-   css selector ✔️
-   link text ✔️
-   partial link text ✔️
-   xpath ✔️
:::
:::

------------

All of these work:

```{r}
remote_driver$
  findElement("link text", "Contributors")$
  clickElement()

remote_driver$
  findElement("partial link text", "Contributors")$
  clickElement()

remote_driver$
  findElement("xpath", "/html/body/div/div[1]/div[1]/div/div[1]/ul/li[3]/a")$
  clickElement()

remote_driver$
  findElement("css selector", "div.col-xs-6:nth-child(1) > ul:nth-child(6) > li:nth-child(3) > a:nth-child(1)")$
  clickElement()
```

. . .

::: callout-tip
You can check that you found the right element by highlighting it with
`highlightElement()`.
:::

------------

We are now on the right page!

![](img/contributors.png)

------------

<br>

Last step: obtain the HTML of the page.

```{r}
remote_driver$getPageSource()
```

<br>

. . .

To read it with `{rvest}`:

```{r}
x <- remote_driver$getPageSource()[[1]]
rvest::read_html(x)
```

------------

##  {#contributors-last-step}

Do we read the HTML and extract the information in the same script?

. . .

**No!**

Instead, we save the HTML in an external file, and we will be able to
access it in another script (and offline) to manipulate it as we want.

<!-- ^[Although, in this case, it wouldn't cost too much to treat it directly in the same script.] -->

```{r}
write(x, file = "contributors.html")
# Later and in another script
rvest::read_html("contributors.html")
```

<br>

Click [here](#contributors-results) to see the results.

# Exercise 2: a harder & real-life example

------------

The previous example was not a *dynamic* page: we could have used the
link to the page and apply webscraping methods for static webpages.

<br>

```{r}
rvest::read_html("https://www.r-project.org/contributors.html")
```

. . .

<br>

Let's now dive into a more complex example, where RSelenium is the only
way to obtain the data.

------------

## Before using RSelenium

*Using RSelenium is slower than using "classic" scraping methods*, so
it's important to check all possibilities before using it.

<br>

. . .

Use Selenium if:

-   the HTML you want is not directly accessible, i.e needs some
    interactions (clicking on a button, connect to a website...),

-   the URL doesn't change with the inputs,

-   you can't access the data directly in the "network" tab of the
    console and you can't reproduce the `POST` request.


------------

## Example: Sao Paulo immigration museum

<br>

[Open website](http://www.inci.org.br/acervodigital/livros.php)

<br>

Steps:

1. list all interactions we need to do
1. check that we need Selenium 
1. make an example
1. generalize and polish the code


------------

## Steps

<br>

1. Open the website
1. Enter "PORTUGUESA" in the input box
1. Wait a bit for the page to load
1. Open every modal "Ver Mais"


------------

## Check that we need Selenium

1. Is there an API?

::: {.fragment .fade-in fragment-index=1}
::: {.fragment .highlight-current-blue fragment-index=1}
Not that I know of (and let's assume that there isn't one).
:::
:::

2. Does the URL change when we enter inputs or click somewhere?

::: {.fragment .fade-in fragment-index=2}
::: {.fragment .highlight-current-blue fragment-index=2}
No.
:::
:::

3. Can we get the data through the "Network" tab?

::: {.fragment .fade-in fragment-index=3}
::: {.fragment .highlight-current-blue fragment-index=3}
Yes but we still need RSelenium to change pages (and this is just training
anyway).
:::
:::


------------

## Make an example {auto-animate="true"}

Initiate the remote driver and go to the website:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)
```


## Make an example {auto-animate=true}

Fill the field "NACIONALIDADE":

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))
```

## Make an example {auto-animate=true}

Find the button "Pesquisar" and click it:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()
```


## Make an example {auto-animate=true}

Find the button "Ver Mais" and click it:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()

# Find the button "Ver Mais" and click it
remote_driver$
  findElement(using = 'id', value = "link_ver_detalhe")$
  clickElement()
```


## Make an example {auto-animate=true}

Get the HTML that is displayed:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"))
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()

# Find the button "Ver Mais" and click it
remote_driver$
  findElement(using = 'id', value = "link_ver_detalhe")$
  clickElement()

# Get the HTML that is displayed in the modal
x <- remote_driver$getPageSource()
```


------------

## Error handling


------------

## Loading times

<br>

After clicking on "Pesquisar", there's a small loading time so **we need to wait** 
before running the rest of the script. 

. . .

<br>

Our best friend in webscraping is `Sys.sleep()`.


# Thanks!

Source code for slides and exercises: 

[https://github.com/etiennebacher/webscraping-teaching](https://github.com/etiennebacher/webscraping-teaching)



# Appendix

------------

## Appendix {#contributors-results}

For reference, here's the code to extract the list of contributors:

```{r appendix, echo=TRUE}
library(rvest)

html <- read_html("contributors.html") 

bullet_points <- html %>% 
  html_elements(css = "div.col-xs-12 > ul > li") %>% 
  html_text()

blockquote <- html %>% 
  html_elements(css = "div.col-xs-12.col-sm-7 > blockquote") %>% 
  html_text() %>% 
  strsplit(., split = ", ")

blockquote <- blockquote[[1]] %>% 
  gsub("\\r|\\n|\\.|and", "", .)

others <- html %>% 
  html_elements(xpath = "/html/body/div/div[1]/div[2]/p[5]") %>% 
  html_text() %>% 
  strsplit(., split = ", ")

others <- others[[1]] %>% 
  gsub("\\r|\\n|\\.|and", "", .)

all_contributors <- c(bullet_points, blockquote, others)
```

------------

## Appendix

```{r appendix, eval=TRUE, echo = FALSE}
```

```{r eval=TRUE, echo = FALSE}
all_contributors[1:136] 
```

[Back](#contributors-last-step)


------------

## Appendix

<br>

Bonus: get the data from each modal on the museum website by performing the `POST`
request yourself.

<br>

Go to the tab "Network" in the developer console, and then click on one of the
"Ver Mais" button to display the modal.

------------

## Appendix

Clicking on this button triggers two `POST` requests: 

* one for the individual information;
* one for the "network" of the individual.

![](img/console_network.png)

------------

## Appendix

<br>

Clicking on one of the `POST` requests displays important information: 

* the **request**, what we send to the server;
* the **response**, what the server sends back to us.

. . .

The request contains specific parameters needed to tell the server which data
we need.

------------

## Appendix {auto-animate=true}

Let's rebuild this `POST` request from R using the package `httr`.

```{r}
library(httr)

x <- POST(
  "http://www.arquivoestado.sp.gov.br/site/acervo/memoria_do_imigrante/getHospedariaDetalhe",
  body = list(
    id = "92276"
  ),
  encode = "multipart"
)
```

------------

## Appendix {auto-animate=true}

Extract the data from the server response:

```{r}
library(httr)
library(xml2)
library(jsonlite)

# make the POST request with the parameters needed
x <- POST(
  "http://www.arquivoestado.sp.gov.br/site/acervo/memoria_do_imigrante/getHospedariaDetalhe",
  body = list(
    id = "92276"
  ),
  encode = "multipart"
)

# convert output to a list
out <- as_list(content(x))

# convert output to a dataframe
fromJSON(unlist(out))$dados
```

------------

## Your turn

<br>

Do the same thing with the second `POST` request, which has 3 parameters 
instead of one.
